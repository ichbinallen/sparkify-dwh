{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sparkify Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a data warehouse for the million song dataset and Eventsim\n",
    "\n",
    "This notebook implements the ETL needed for the Sparkify Cloud Data Warehouse.  Sparkify is a ficticious music streaming app based on data from the following sources.\n",
    " * [Million Song Dataset](http://millionsongdataset.com/)\n",
    " * [EventSim](https://github.com/Interana/eventsim)\n",
    " \n",
    "The cloud data warehouse is hosted on AWS.  JSON log files from the EventSim are stored in an AWS S3 bucket hosted by Udacity.\n",
    "This project performs ETL processes to place data in a star schema in AWS Redshift for efficient OLAP use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read Configuration File\n",
    "\n",
    "The configuration file stores some database meta data.  Here we read it in to python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY=config.get('IAM_ROLE','key')\n",
    "SECRET= config.get('IAM_ROLE','secret')\n",
    "\n",
    "DWH_CLUSTER_TYPE=config.get(\"SETUP\", \"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES=config.get(\"SETUP\", \"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE=config.get(\"SETUP\", \"DWH_NODE_TYPE\")\n",
    "DWH_IAM_ROLE_NAME=config.get(\"SETUP\", \"DWH_IAM_ROLE_NAME\")\n",
    "DWH_CLUSTER_IDENTIFIER=config.get(\"SETUP\", \"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB=config.get(\"CLUSTER\", \"DB_NAME\")\n",
    "DWH_DB_USER=config.get(\"CLUSTER\", \"DB_USER\")\n",
    "DWH_DB_PASSWORD=config.get(\"CLUSTER\", \"DB_PASSWORD\")\n",
    "DWH_PORT=config.get(\"CLUSTER\", \"DB_PORT\")\n",
    "DB_HOST = config.get(\"CLUSTER\", \"HOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create AWS Resources\n",
    "\n",
    "We need an EC2 server for the ETL, an S3 resource to read the log_data from the S3 bucket's json files, and a redshift cluster to host the DWH.\n",
    "\n",
    "Of course, this also requires an IAM role from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource(\n",
    "    \"ec2\",\n",
    "    region_name=\"us-west-2\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    \"s3\",\n",
    "    region_name=\"us-west-2\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "iam = boto3.client(\n",
    "    \"iam\",\n",
    "    region_name=\"us-west-2\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "redshift = boto3.client(\n",
    "    \"redshift\",\n",
    "    region_name=\"us-west-2\",\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### JSON Log Bucket\n",
    "\n",
    "Let's check out the S3 Bucket and confirm the event logs, song files, and jsonpath files are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Log Files:\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-01-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-02-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-03-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-04-events.json')\n",
      "Song Log Files:\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAK128F9318786.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAV128F421A322.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAABD128F429CF47.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAACN128F9355673.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAEA128F935A30D.json')\n"
     ]
    }
   ],
   "source": [
    "log_bucket = s3.Bucket(\"udacity-dend\")\n",
    "a_few_logs = [obj for obj in log_bucket.objects.filter(Prefix=\"log-data/\")][0:5]\n",
    "print(\"Event Log Files:\")\n",
    "for el in a_few_logs:\n",
    "    print(el)\n",
    "a_few_songs = [obj for obj in log_bucket.objects.filter(Prefix=\"song_data/A/A/A\")][0:5]\n",
    "print(\"Song Log Files:\")\n",
    "for sl in a_few_songs:\n",
    "    print(sl)\n",
    "    \n",
    "log_bucket.download_file(\"log_json_path.json\", \"udacity_jsonpaths_file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Get ARN Role\n",
    "\n",
    "Here we can create a new iam role for interacting with redshift, attach it to our boto3 `iam` client, and get the ARN for this role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "    Path='/',\n",
    "    RoleName=DWH_IAM_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'}),\n",
    "    Description='Allows Redshift clusters to call AWS services on your behalf.',\n",
    ")\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "# Attach The role to the iam object\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Launch Redshift cluster\n",
    "\n",
    "Now we are ready to launch the redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #Hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "         \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.ck2nmy7mqyc7.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-500f6f28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  available                                                                              \n",
       "3  dwhuser                                                                                \n",
       "4  dwhcluster                                                                             \n",
       "5  {'Address': 'dwhcluster.ck2nmy7mqyc7.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-500f6f28                                                                           \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "! Don't run until cluster is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DWH_ENDPOINT = \"dwhcluster.ck2nmy7mqyc7.us-west-2.redshift.amazonaws.com:5439/dwh\"\n",
    "#DWH_HOST = \"dwhcluster.ck2nmy7mqyc7.us-west-2.redshift.amazonaws.com\"\n",
    "# Following line of code not working, get this manually from AWS console\n",
    "# DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "#DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "#print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "#print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Next it is time to open a TCP port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-d80b6284')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName='default',\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Test the DB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwhcluster'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DB_HOST, DWH_PORT,DWH_DB)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### CREATE TABLES\n",
    "\n",
    "We'll create the tables needed on the redshift cluster by executing the `create_tables.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ck2nmy7mqyc7.us-west-2.redshift.amazonaws.com:5439/dwhcluster\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>songplay_id</th>\n",
       "        <th>start_time</th>\n",
       "        <th>user_id</th>\n",
       "        <th>level</th>\n",
       "        <th>song_id</th>\n",
       "        <th>artist_id</th>\n",
       "        <th>session_id</th>\n",
       "        <th>location</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after `python create_tables.py`\n",
    "%sql SELECT * FROM songplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ETL\n",
    "We'll execute the `etl.py` script to copy json files from s3 to redshift\n",
    "and transform log data to star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COPY staging_events\n",
      "FROM 's3://udacity-dend/log_data'\n",
      "IAM_ROLE AS 'aws_iam_role=arn:aws:iam::747157885091:role/dwhRole'\n",
      "JSON 's3://udacity-dend/log_json_path.json'\n",
      "REGION 'us-west-2'\n",
      ";\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"etl.py\", line 36, in <module>\n",
      "    main()\n",
      "  File \"etl.py\", line 29, in main\n",
      "    load_staging_tables(cur, conn)\n",
      "  File \"etl.py\", line 9, in load_staging_tables\n",
      "    cur.execute(query)\n",
      "psycopg2.InternalError: User arn:aws:redshift:us-west-2:747157885091:dbuser:dwhcluster/dwhuser is not authorized to assume IAM Role aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
      "DETAIL:  \n",
      "  -----------------------------------------------\n",
      "  error:  User arn:aws:redshift:us-west-2:747157885091:dbuser:dwhcluster/dwhuser is not authorized to assume IAM Role aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
      "  code:      8001\n",
      "  context:   IAM Role=aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
      "  query:     506\n",
      "  location:  xen_aws_credentials_mgr.cpp:324\n",
      "  process:   padbmaster [pid=13779]\n",
      "  -----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python etl.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "root@746d14f8f268:/home/workspace# python etl.py\n",
    "\n",
    "COPY staging_events\n",
    "FROM 's3://udacity-dend/log_data'\n",
    "IAM_ROLE AS 'aws_iam_role=arn:aws:iam::747157885091:role/dwhRole'\n",
    "JSON 's3://udacity-dend/log_json_path.json'\n",
    "REGION 'us-west-2'\n",
    ";\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"etl.py\", line 36, in <module>\n",
    "    main()\n",
    "  File \"etl.py\", line 29, in main\n",
    "    load_staging_tables(cur, conn)\n",
    "  File \"etl.py\", line 9, in load_staging_tables\n",
    "    cur.execute(query)\n",
    "psycopg2.InternalError: User arn:aws:redshift:us-west-2:747157885091:dbuser:dwhcluster/dwhuser is not authorized to assume IAM Role aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
    "DETAIL:  \n",
    "  -----------------------------------------------\n",
    "  error:  User arn:aws:redshift:us-west-2:747157885091:dbuser:dwhcluster/dwhuser is not authorized to assume IAM Role aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
    "  code:      8001\n",
    "  context:   IAM Role=aws_iam_role=arn:aws:iam::747157885091:role/dwhRole\n",
    "  query:     410\n",
    "  location:  xen_aws_credentials_mgr.cpp:324\n",
    "  process:   padbmaster [pid=14096]\n",
    "  -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_events LIMIT 20;\n",
    "# misc sql queries, test drop/create tables, copy from JSON, INSERT INTO queries, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back to the Notebook\n",
    "\n",
    "One all the ETL is completed, we can shut down the redshift cluster to save costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
